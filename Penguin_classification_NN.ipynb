{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following on from our data exploration exercise in the last file 'Penguin_classification_explore', we will\n",
    "# apply what we've learned from the data to train a neural network and evaluate it with\n",
    "# cross-validation (CV). This should help get a better understanding of how well the model generalises to unseesn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae67406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#matplotlib inline\n",
    "\n",
    "# load the dataset\n",
    "penguins = pd.read_csv('./penguins_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa08edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use the Keras and sklearn libraries to perform CV\n",
    "# so we can pre-process our data as before. We'll remove much of the code annotation here (see previous notebook)\n",
    "\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "# for i in range(1,3):\n",
    "#     penguins = penguins.append(penguins)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# transform all categorical features\n",
    "# ct = ColumnTransformer([(\"island\", OneHotEncoder(),['island']),(\"species\", OneHotEncoder(),['species']),\\\n",
    "#                        (\"sex\", OneHotEncoder(),['sex'])],remainder=\"passthrough\")\n",
    "ct = ColumnTransformer([(\"island\", OneHotEncoder(),['island']),(\"species\", OneHotEncoder(),['species']),\\\n",
    "                       (\"sex\", OneHotEncoder(),['sex'])],remainder=\"passthrough\")\n",
    "penguins_transformed=ct.fit_transform(penguins) # data is now in array form  \n",
    "\n",
    "transformed_columns=ct.get_feature_names()\n",
    "penguins_transformed_df=pd.DataFrame(penguins_transformed,columns=transformed_columns) \n",
    "\n",
    "penguin_classes=[]\n",
    "penguin_classes_trans=transformed_columns[3:6]\n",
    "for i in range(len(penguin_classes_trans)):\n",
    "    penguin_classes.append(penguin_classes_trans[i][12:]) # track the class ordering for later assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e700bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# before scaling, let's split the data first to avoid information leakage.\n",
    "X=penguins_transformed_df.drop(columns=['species__x0_Adelie','species__x0_Chinstrap',\\\n",
    "                                      'species__x0_Gentoo','sex__x0_.']) # features\n",
    "y=penguins_transformed_df[['species__x0_Adelie','species__x0_Chinstrap','species__x0_Gentoo']] # labels\n",
    "\n",
    "# we'll use a 80:10:10 split for our training:validation:testing datasets \n",
    "# to this end we'll use train_test_split twice\n",
    "train_size=0.8\n",
    "\n",
    "# split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# split remaining in to validation and testing sets\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b18623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we can now re-scale the remaining non-categorical features to avoid unequal weighting in the ML algorithms\n",
    "\n",
    "# need dataframe copies for normalisation and standardisation scaling\n",
    "X_train_norm=X_train.copy()\n",
    "X_valid_norm=X_valid.copy()\n",
    "X_test_norm=X_test.copy()\n",
    "X_train_scale=X_train.copy()\n",
    "X_valid_scale=X_valid.copy()\n",
    "X_test_scale=X_test.copy()\n",
    "\n",
    "# fit scaler on non-categorical training data\n",
    "norm = MinMaxScaler().fit(X_train[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']])\n",
    "# transform training data\n",
    "X_train_norm_noncat = norm.transform(X_train_norm[['culmen_length_mm','culmen_depth_mm','flipper_length_mm',\\\n",
    "                                                   'body_mass_g']])\n",
    "# transform validation data\n",
    "X_valid_norm_noncat = norm.transform(X_valid_norm[['culmen_length_mm','culmen_depth_mm','flipper_length_mm',\\\n",
    "                                                   'body_mass_g']])\n",
    "# transform testing data\n",
    "X_test_norm_noncat = norm.transform(X_test[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']])\n",
    "# need to recombine scaled data with categorical data\n",
    "# we only actually need to use the copied datasets here when recombining as the above df slices didnt\n",
    "# affect the dfs themselves\n",
    "X_train_norm[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_train_norm_noncat\n",
    "X_valid_norm[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_valid_norm_noncat\n",
    "X_test_norm[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_test_norm_noncat\n",
    "\n",
    "# let's also standardise our data for comparison\n",
    "scale = StandardScaler().fit(X_train[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']])\n",
    "X_train_scale_noncat = scale.transform(X_train_scale[['culmen_length_mm','culmen_depth_mm',\\\n",
    "                                                'flipper_length_mm','body_mass_g']])\n",
    "X_valid_scale_noncat = scale.transform(X_valid_scale[['culmen_length_mm','culmen_depth_mm',\\\n",
    "                                                'flipper_length_mm','body_mass_g']])\n",
    "X_test_scale_noncat = scale.transform(X_test_scale[['culmen_length_mm','culmen_depth_mm',\\\n",
    "                                                    'flipper_length_mm','body_mass_g']])\n",
    "X_train_scale[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_train_scale_noncat\n",
    "X_valid_scale[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_valid_scale_noncat\n",
    "X_test_scale[['culmen_length_mm','culmen_depth_mm','flipper_length_mm','body_mass_g']]\\\n",
    "    =X_test_scale_noncat\n",
    "\n",
    "# # it turns out, cross_val_score cannot work with multi-class targets so we'll actually return our y_train to a single\n",
    "# # 1D-array of labels\n",
    "\n",
    "# y_train=y_train.idxmax(axis=1) # this line needs to be turned off when not using GridSearchCV and cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f76cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data is preprocessed and ready to be used to train some models\n",
    "# we'll begin with a neural network\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "seed=numpy.random.seed(7) # Set random seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d71a14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create model instances. This is required for the KerasClassifer as it will need to create\n",
    "# new instances for each validation pass.\n",
    "\n",
    "hl = 10 # Number of hidden layer nodes\n",
    "\n",
    "num_features=X_train.shape[1]\n",
    "num_penguin_classes=len(penguin_classes)\n",
    "\n",
    "# NN creater function for CV and final testing\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hl, input_dim=num_features, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    model.add(Dense(hl, input_dim=hl,kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    model.add(Dense(num_penguin_classes,kernel_initializer='glorot_uniform', input_dim=hl, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# # NN creater function for GridSearch\n",
    "# def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(hl, input_dim=num_features,kernel_initializer=init, activation='relu'))\n",
    "#     model.add(Dense(hl, input_dim=hl,kernel_initializer=init, activation='relu'))\n",
    "#     model.add(Dense(num_penguin_classes,kernel_initializer=init, input_dim=hl, activation='softmax'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9946a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8038686af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8038c99160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.9962962985038757\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X_train_norm, y_train, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d8cffb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 1.000000 using {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.988819 (0.014850) with: {'batch_size': 5, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.984976 (0.014101) with: {'batch_size': 5, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.861006 (0.077921) with: {'batch_size': 5, 'epochs': 25, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.853599 (0.052891) with: {'batch_size': 5, 'epochs': 25, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.809015 (0.052120) with: {'batch_size': 5, 'epochs': 25, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.868833 (0.068599) with: {'batch_size': 5, 'epochs': 25, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.996296 (0.007407) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "1.000000 (0.000000) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.958770 (0.049747) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.966108 (0.049798) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.925157 (0.086440) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.925087 (0.086395) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.992523 (0.009158) with: {'batch_size': 5, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.992523 (0.009158) with: {'batch_size': 5, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.988819 (0.009130) with: {'batch_size': 5, 'epochs': 75, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.992523 (0.009158) with: {'batch_size': 5, 'epochs': 75, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.966108 (0.049798) with: {'batch_size': 5, 'epochs': 75, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.951013 (0.079720) with: {'batch_size': 5, 'epochs': 75, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.913767 (0.042513) with: {'batch_size': 10, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.981342 (0.016721) with: {'batch_size': 10, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.790147 (0.040407) with: {'batch_size': 10, 'epochs': 25, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.831656 (0.080236) with: {'batch_size': 10, 'epochs': 25, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.775402 (0.067119) with: {'batch_size': 10, 'epochs': 25, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.793920 (0.043005) with: {'batch_size': 10, 'epochs': 25, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.996226 (0.007547) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.992523 (0.009158) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.921454 (0.084318) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.954787 (0.081483) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.801188 (0.040055) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.913767 (0.100097) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.996226 (0.007547) with: {'batch_size': 10, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.996226 (0.007547) with: {'batch_size': 10, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.966108 (0.049798) with: {'batch_size': 10, 'epochs': 75, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.996296 (0.007407) with: {'batch_size': 10, 'epochs': 75, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.939693 (0.073018) with: {'batch_size': 10, 'epochs': 75, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.921663 (0.077438) with: {'batch_size': 10, 'epochs': 75, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.873305 (0.104946) with: {'batch_size': 20, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.865059 (0.068911) with: {'batch_size': 20, 'epochs': 25, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.719357 (0.031034) with: {'batch_size': 20, 'epochs': 25, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.775122 (0.059787) with: {'batch_size': 20, 'epochs': 25, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.730538 (0.058914) with: {'batch_size': 20, 'epochs': 25, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.771488 (0.030122) with: {'batch_size': 20, 'epochs': 25, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.940112 (0.032362) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.992523 (0.009158) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.809015 (0.052120) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.861845 (0.105517) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.846751 (0.088739) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.861356 (0.088149) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.992523 (0.009158) with: {'batch_size': 20, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.988819 (0.014850) with: {'batch_size': 20, 'epochs': 75, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.883788 (0.083804) with: {'batch_size': 20, 'epochs': 75, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.951083 (0.080602) with: {'batch_size': 20, 'epochs': 75, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.891754 (0.083152) with: {'batch_size': 20, 'epochs': 75, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.917680 (0.081078) with: {'batch_size': 20, 'epochs': 75, 'init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# the output from our CV is 0.99 i.e. 99% of the time the model was correct  so pretty good. \n",
    "# let's grid search the hyperparameters to try improve on this\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [25, 50, 75]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train_norm, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bce2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the optimum hyperparameters, at least for the few we searched, are\n",
    "# {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3604347",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.0430 - accuracy: 0.5655 - val_loss: 0.9830 - val_accuracy: 0.7273\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9730 - accuracy: 0.6554 - val_loss: 0.9105 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.9121 - accuracy: 0.6554 - val_loss: 0.8540 - val_accuracy: 0.6970\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.8580 - accuracy: 0.6554 - val_loss: 0.7963 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 934us/step - loss: 0.8045 - accuracy: 0.6554 - val_loss: 0.7429 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 963us/step - loss: 0.7534 - accuracy: 0.6554 - val_loss: 0.6930 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 940us/step - loss: 0.7100 - accuracy: 0.6554 - val_loss: 0.6525 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 939us/step - loss: 0.6702 - accuracy: 0.6554 - val_loss: 0.6148 - val_accuracy: 0.6970\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 991us/step - loss: 0.6324 - accuracy: 0.6592 - val_loss: 0.5782 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 933us/step - loss: 0.5970 - accuracy: 0.6592 - val_loss: 0.5443 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.5642 - accuracy: 0.6742 - val_loss: 0.5110 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 949us/step - loss: 0.5317 - accuracy: 0.6966 - val_loss: 0.4777 - val_accuracy: 0.7273\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 942us/step - loss: 0.5000 - accuracy: 0.7228 - val_loss: 0.4459 - val_accuracy: 0.7576\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 960us/step - loss: 0.4673 - accuracy: 0.7566 - val_loss: 0.4123 - val_accuracy: 0.8182\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 941us/step - loss: 0.4348 - accuracy: 0.8277 - val_loss: 0.3840 - val_accuracy: 0.8182\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 957us/step - loss: 0.4068 - accuracy: 0.8727 - val_loss: 0.3539 - val_accuracy: 0.8485\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 951us/step - loss: 0.3777 - accuracy: 0.9363 - val_loss: 0.3273 - val_accuracy: 0.9697\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 925us/step - loss: 0.3515 - accuracy: 0.9588 - val_loss: 0.3014 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 977us/step - loss: 0.3287 - accuracy: 0.9625 - val_loss: 0.2796 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 929us/step - loss: 0.3060 - accuracy: 0.9775 - val_loss: 0.2584 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 991us/step - loss: 0.2864 - accuracy: 0.9663 - val_loss: 0.2412 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 946us/step - loss: 0.2691 - accuracy: 0.9813 - val_loss: 0.2276 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9850 - val_loss: 0.2120 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2389 - accuracy: 0.9850 - val_loss: 0.1988 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9775 - val_loss: 0.1920 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9775 - val_loss: 0.1826 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 963us/step - loss: 0.2071 - accuracy: 0.9850 - val_loss: 0.1713 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 954us/step - loss: 0.1974 - accuracy: 0.9813 - val_loss: 0.1616 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 910us/step - loss: 0.1825 - accuracy: 0.9850 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 964us/step - loss: 0.1709 - accuracy: 0.9888 - val_loss: 0.1473 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9850 - val_loss: 0.1326 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9888 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 960us/step - loss: 0.1466 - accuracy: 0.9888 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 957us/step - loss: 0.1389 - accuracy: 0.9888 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 957us/step - loss: 0.1310 - accuracy: 0.9850 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 956us/step - loss: 0.1233 - accuracy: 0.9888 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 926us/step - loss: 0.1160 - accuracy: 0.9850 - val_loss: 0.0927 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 948us/step - loss: 0.1092 - accuracy: 0.9850 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 947us/step - loss: 0.1023 - accuracy: 0.9888 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 935us/step - loss: 0.0969 - accuracy: 0.9888 - val_loss: 0.0793 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 963us/step - loss: 0.0925 - accuracy: 0.9850 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 975us/step - loss: 0.0885 - accuracy: 0.9850 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 950us/step - loss: 0.0828 - accuracy: 0.9888 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 918us/step - loss: 0.0787 - accuracy: 0.9925 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 941us/step - loss: 0.0758 - accuracy: 0.9888 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9888 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9888 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 943us/step - loss: 0.0624 - accuracy: 0.9925 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 973us/step - loss: 0.0591 - accuracy: 0.9963 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 924us/step - loss: 0.0559 - accuracy: 0.9888 - val_loss: 0.0425 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's a model with these hyperparameters on the hold-out test data\n",
    "\n",
    "# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
    "num_epochs = 50\n",
    "batch_size=10\n",
    "\n",
    "model_norm = create_model()\n",
    "history = model_norm.fit(X_train_norm, y_train, epochs=num_epochs, batch_size=batch_size,\\\n",
    "                         validation_data=(X_valid_norm, y_valid))\n",
    "\n",
    "class_probabilities = model_norm.predict(X_test_norm)\n",
    "\n",
    "predictions = np.argmax(class_probabilities, axis=1) # select highest probability for prediction\n",
    "true_labels = np.argmax(y_test.values, axis=1) # select \n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=85)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Actual Class\")\n",
    "plt.ylabel(\"Predicted Class\")\n",
    "plt.show()\n",
    "\n",
    "rmse_norm=np.sqrt(mean_squared_error(true_labels,predictions))\n",
    "rmse_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7c906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
